{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-07-28T04:32:14.407538Z","iopub.status.busy":"2022-07-28T04:32:14.407085Z","iopub.status.idle":"2022-07-28T04:32:17.769130Z","shell.execute_reply":"2022-07-28T04:32:17.768081Z","shell.execute_reply.started":"2022-07-28T04:32:14.407443Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\JonathonCavalieri\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import numpy as np\n","#import scipy\n","import pandas as pd\n","import gc\n","import os, psutil\n","import pickle\n","\n","from sklearn.base import clone\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","import optuna\n","from optuna import Trial\n","from optuna.samplers import TPESampler\n","\n","from xgboost import XGBClassifier, plot_importance\n","from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n","\n","import joblib\n","\n","from warnings import simplefilter\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","pd.set_option('mode.chained_assignment', None)"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:32:17.771465Z","iopub.status.busy":"2022-07-28T04:32:17.771085Z","iopub.status.idle":"2022-07-28T04:32:17.779961Z","shell.execute_reply":"2022-07-28T04:32:17.776000Z","shell.execute_reply.started":"2022-07-28T04:32:17.771436Z"},"trusted":true},"outputs":[],"source":["base_dir =  'kaggle/input/amex-default-prediction/'\n","filename = base_dir + 'train_data.csv'\n","filename_label = base_dir + 'train_labels.csv'\n","filename_test = base_dir + 'test_data.csv'\n","\n","model_save_path = 'model\\\\'\n","\n","random_state = 42"]},{"cell_type":"markdown","metadata":{},"source":["## Functions for amex comp "]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:32:19.898216Z","iopub.status.busy":"2022-07-28T04:32:19.897544Z","iopub.status.idle":"2022-07-28T04:32:19.910008Z","shell.execute_reply":"2022-07-28T04:32:19.908774Z","shell.execute_reply.started":"2022-07-28T04:32:19.898177Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory used Process: 0.2GB Memory used Total: 18.3GB\n"]},{"data":{"text/plain":["0.18233108520507812"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["def memory_usage(metric='MB', places=1,print_out=True):\n","    metric_mapping = {'B':0, 'KB': 2, 'MB': 2, 'GB': 3}\n","    multiplier= metric_mapping[metric]\n","    \n","    mem_used = psutil.Process(os.getpid()).memory_info().rss / 1024 ** multiplier\n","    mem_used_total = psutil.virtual_memory()[3] /1024**multiplier\n","    if print_out:\n","        print(f'Memory used Process: {mem_used:.{places}F}{metric} Memory used Total: {mem_used_total:.{places}F}{metric}')\n","\n","    return mem_used\n","memory_usage(\"GB\",1)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:42:35.266791Z","iopub.status.busy":"2022-07-28T04:42:35.266337Z","iopub.status.idle":"2022-07-28T04:42:35.281458Z","shell.execute_reply":"2022-07-28T04:42:35.280333Z","shell.execute_reply.started":"2022-07-28T04:42:35.266748Z"},"trusted":true},"outputs":[],"source":["def amex_metric_numpy(y_true: np.array, y_pred: np.array) -> float:\n","    #from here https://www.kaggle.com/code/rohanrao/amex-competition-metric-implementations\n","    # count of positives and negatives\n","    n_pos = y_true.sum()\n","    n_neg = y_true.shape[0] - n_pos\n","\n","    # sorting by descring prediction values\n","    indices = np.argsort(y_pred)[::-1]\n","    preds, target = y_pred[indices], y_true[indices]\n","\n","    # filter the top 4% by cumulative row weights\n","    weight = 20.0 - target * 19.0\n","    cum_norm_weight = (weight / weight.sum()).cumsum()\n","    four_pct_filter = cum_norm_weight <= 0.04\n","\n","    # default rate captured at 4%\n","    d = target[four_pct_filter].sum() / n_pos\n","\n","    # weighted gini coefficient\n","    lorentz = (target / n_pos).cumsum()\n","    gini = ((lorentz - cum_norm_weight) * weight).sum()\n","\n","    # max weighted gini coefficient\n","    gini_max = 10 * n_neg * (1 - 19 / (n_pos + 20 * n_neg))\n","\n","    # normalized weighted gini coefficient\n","    g = gini / gini_max\n","\n","    return 0.5 * (g + d)\n","\n","def eval_model_classification_report(x_test, y_test, models):\n","    actual = np.array(y_test).squeeze()\n","    \n","    if isinstance(models, list):\n","        preds = np.zeros(len(x_test))\n","        for model in models: \n","            preds += model.predict_proba(x_test)[:,1]\n","        #weight each one equally\n","        preds /= len(models)\n","        y_preds = (preds >0.5).astype(np.int8)\n","        \n","        \n","    else:\n","        preds = models.predict_proba(x_test)[:,1]\n","        y_pred = models.predict(x_test)\n","    \n","    amex_score = amex_metric_numpy(actual,preds)\n","\n","    \n","    print(f'Validation Results - Amex metric: {amex_score:.3f} \\n\\nClassification Report\\n')\n","    print(classification_report(y_test, y_pred, target_names=['No Default','Default']))\n","    print('\\n\\nConfusion Matrix\\n')\n","#     print(confusion_matrix(y_test, y_pred))\n","    cmtx = pd.DataFrame(\n","        confusion_matrix(y_test, y_pred), \n","        index=['true:yes', 'true:no'], \n","        columns=['pred:yes', 'pred:no']\n","    )\n","    print(cmtx)\n","\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Helper class from prep notebook for scoring"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:32:24.439601Z","iopub.status.busy":"2022-07-28T04:32:24.439249Z","iopub.status.idle":"2022-07-28T04:32:24.522093Z","shell.execute_reply":"2022-07-28T04:32:24.520916Z","shell.execute_reply.started":"2022-07-28T04:32:24.439570Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["#custom aggregate functions\n","def lm_diff(series):\n","    if len(series)>1:\n","        return series.iloc[-1] - series.iloc[-2]\n","    else:\n","        return 0\n","\n","def squared_mean(series):\n","    return (series**2).mean()\n","\n","def missing_values(series):\n","    return series.isna().sum()\n","\n","def missing_last_value(series):\n","    return series.isna().sum()\n","\n","def missing_last_value(series):\n","    return series.isna().iloc[-1].astype(int)\n","\n","class amex_helper():\n","      \n","    def __init__(self, chunksize= 500000):\n","        self.previous_chunk_data= None\n","        self.chunksize = chunksize\n","        self.key_columns = ['customer_ID','S_2']\n","        self.categorical_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","        self.non_numeric_columns = self.key_columns + self.categorical_columns\n","        \n","        self.numeric_dtype = np.float32\n","        \n","        #Values gotten from for converting to inshttps://www.kaggle.com/code/raddar/amex-data-int-types-train/notebook\n","        self.int_scaling = [\n","            ('B_4',0,78), ('B_16',0,12), ('B_19',0,1), ('B_20',0,17), ('B_22',0,2), ('B_31',0,1), ('B_32',0,1), ('B_33',0,1), ('B_41',0,1), \n","            ('D_39',0,34), ('D_44',0,8), ('D_49',0,71), ('D_51',0,3), ('D_59',48/5,48), ('D_65',0,38), ('D_70',0,4), \n","            ('D_72',0,3), ('D_74',0,14), ('D_75',0,15), ('D_78',0,2), ('D_79',0,2), ('D_80',0,5), ('D_81',0,1), ('D_82',0,2), \n","            ('D_83',0,1), ('D_84',0,2), ('D_86',0,1), ('D_87',0,1), ('D_89',0,9), ('D_91',0,2), ('D_92',0,1), ('D_93',0,1), \n","            ('D_94',0,1), ('D_96',0,1), ('D_103',0,1), ('D_106',0,23), ('D_107',0,3), ('D_108',0,1), ('D_109',0,1),('D_111',0,2),\n","            ('D_113',0,5), ('D_122',0,7), ('D_123',0,1), ('D_124',22,22), ('D_125',0,1), ('D_127',0,1), ('D_129',0,1), ('D_135',0,1),\n","            ('D_136',0,4), ('D_137',0,1), ('D_138',0,2), ('D_139',0,1), ('D_140',0,1), ('D_143',0,1), ('D_145',0,11), ('R_2',0,1), \n","            ('R_3',0,10), ('R_4',0,1), ('R_5',0,2),('R_8',0,1), ('R_9',0,6), ('R_10',0,1),('R_11',0,2), ('R_13',0,31), \n","            ('R_15',0,1), ('R_16',0,2), ('R_17',0,35), ('R_18',0,31),('R_19',0,1), ('R_20',0,1), ('R_21',0,1),('R_22',0,1), \n","            ('R_23',0,1), ('R_24',0,1), ('R_25',0,1), ('R_26',0,28), ('R_28',0,1), ('S_6',0,1), ('S_11',5,25), ('S_15',10/3,10),\n","            ('S_18',0,1), ('S_20',0,1),\n","            ('S_8',0,100), ('S_13',0,100) # these have overlap and should be treated differently\n","              ]\n","    \n","    def read_data(self, filename, update_columns=True, first_chunk_only=False, raw=False):\n","\n","        print(f'Reading Data ')\n","        if update_columns:\n","            #read all columns names and update lists\n","            self.all_columns = pd.read_csv(filename, index_col=False, nrows=0).columns.tolist()\n","            self.numeric_columns = [x for x in self.all_columns if x not in self.non_numeric_columns]\n","            #Types of features\n","            # self.delinquency_features = [col for col in self.all_columns if col.startswith('D_')]\n","            # self.spend_features = [col for col in self.all_columns if col.startswith('S_')]\n","            # self.payment_features = [col for col in self.all_columns if col.startswith('P_')]\n","            # self.balance_features = [col for col in self.all_columns if col.startswith('B_')]\n","            # self.risk_features = [col for col in self.all_columns if col.startswith('R_')]\n","            \n","            print(f'Calculating data sets aggregate statistics')\n","            aggregate_final = None\n","            final_column_na = None\n","            final_count_values = []\n","            drop_cuttoff = 0.35\n","            \n","            with pd.read_csv(filename, chunksize=400000) as reader: #self.chunksize\n","                for i, chunk in enumerate(reader):\n","                    \n","                    #calculate chunk aggregates\n","                    aggregate = chunk[self.numeric_columns].agg(['mean', 'count', 'min', 'max',squared_mean]).transpose()\n","                    #coutn number of na in chunk\n","                    chunk_column_na = chunk.agg(['count',missing_values]).transpose() \n","                    #get count of each value in categorical columns\n","                    chunk_count_values = dict()\n","                    for column in self.categorical_columns:\n","                        chunk_count_values[column] = chunk[column].value_counts().to_dict()\n","                    final_count_values.append(pd.DataFrame(chunk_count_values))\n","                    \n","                    del chunk\n","                    gc.collect()\n","                    \n","                    #update aggregates\n","                    if aggregate_final is not None:\n","                        aggregate_final['mean'] = (aggregate_final['mean']*aggregate_final['count']  + aggregate['mean']*aggregate['count'] ) / ( aggregate_final['count'] + aggregate['count'] )\n","                        aggregate_final['squared_mean'] = (aggregate_final['squared_mean']*aggregate_final['count']  + aggregate['squared_mean']*aggregate['count'] ) / ( aggregate_final['count'] + aggregate['count'] )\n","                        aggregate_final['count'] = aggregate_final['count'] + aggregate['count']\n","                        aggregate_final['min'] = pd.concat([aggregate_final['min'],  aggregate['min']], axis=1).min(axis=1)\n","                        aggregate_final['max'] = pd.concat([aggregate_final['max'],  aggregate['max']], axis=1).max(axis=1)\n","                    else:\n","                        aggregate_final = aggregate\n","                        \n","                    #update NA values\n","                    if final_column_na is not None:\n","                        final_column_na['count'] = final_column_na['count'] + chunk_column_na['count']\n","                        final_column_na['missing_values'] = final_column_na['missing_values'] + chunk_column_na['missing_values']\n","                        break\n","                    else:\n","                        final_column_na = chunk_column_na\n","                        \n","            #Na Values final \n","            final_column_na['percent_missing'] = final_column_na['missing_values'] / (final_column_na['missing_values'] +final_column_na['count'])\n","            self.columns_drop = list(final_column_na[final_column_na['percent_missing']> drop_cuttoff].reset_index()['index'])\n","            #Add redundant features to drop list \n","            self.columns_drop = list(set(self.columns_drop + ['D_103','D_139'])) #tied to D_107 and D_145 respectively\n","            \n","            #update meta data list for columns to remove columns that will be dropped\n","            self.categorical_columns = list(set(self.categorical_columns) - set(self.columns_drop ) )\n","            self.non_numeric_columns = list(set(self.non_numeric_columns) - set(self.columns_drop))\n","            self.all_columns = list(set(self.all_columns) - set(self.columns_drop))\n","            self.numeric_columns = list(set(self.numeric_columns ) - set(self.columns_drop))\n","            self.int_scaling = [x for x in self.int_scaling if x[0] not in self.columns_drop]\n","            \n","            #aggregates values final\n","            aggregate_final['var'] = aggregate_final['squared_mean'] - aggregate_final['mean']**2      \n","            aggregate_final['std'] = aggregate_final['var']**(1/2)\n","            aggregate_final = aggregate_final[~aggregate_final.index.isin(self.columns_drop)] #remove columns that will be dropped\n","            self.aggregate_stats  = aggregate_final\n","                       \n","            #merge all value counts\n","            final_count_values = pd.concat(final_count_values).groupby(level=0).sum()\n","            #remove columns that will be dropped\n","            final_count_values = final_count_values[~final_count_values.index.isin(self.columns_drop)]\n","            self.categorical_mode = final_count_values.idxmax().to_dict()\n","            #get dict of values to impute categorical values\n","            categorical_values = dict()\n","            for column in final_count_values.columns:\n","                mapping_dict = final_count_values[final_count_values[column] != 0].reset_index()['index'].to_dict()\n","                categorical_values[column] = { mapping_dict[x]:x for x in mapping_dict}\n","            self.categorical_encode = categorical_values\n","        print(f'Reading {filename} in chunks')\n","        if raw:\n","            return pd.read_csv(filename)\n","        else:\n","            output = []\n","            with pd.read_csv(filename, chunksize=self.chunksize) as reader:\n","                for i, chunk in enumerate(reader):\n","                    print(f'Reading chunk: {i+1}')\n","                    output.append(self.process_chunk(chunk))\n","                    memory_usage(\"GB\",1)\n","                    \n","                    if first_chunk_only:\n","                        break\n","                    gc.collect()\n","                    \n","            print(f'finished reading all chunks')\n","            gc.collect()\n","            self.previous_chunk_data = None\n","            print(f'combining all chunks')\n","            data = pd.concat(output, copy=False).sort_index().reset_index(drop=True)\n","            print(f'minimising data types')\n","            self.print_memory_usage(data, label='before')\n","            self.update_dtypes = self.compress(data)\n","            data = data.astype(self.update_dtypes)\n","            self.print_memory_usage(data, label='compressed')\n","            \n","            return data\n","\n","\n","\n","    def process_chunk(self, chunk):\n","\n","        ##Pre Processing of dataframe chunk to make sure customer ID is all processed at same time##\n","\n","        #Test to see if it is not last chunk and take last id from the DF incase it is over 2 chunks\n","        if len(chunk) >= self.chunksize:\n","            last_id_in_chunk = chunk['customer_ID'].iloc[-1]\n","            last_id_in_chunk_data = chunk[chunk['customer_ID']==last_id_in_chunk].copy()\n","            chunk = chunk.loc[chunk['customer_ID']!=last_id_in_chunk]\n","        else: \n","            last_id_in_chunk_data = None\n","\n","        #Check if any previous chunk data\n","        if self.previous_chunk_data is not None: \n","            chunk = pd.concat([self.previous_chunk_data,chunk])\n","        \n","\n","        self.previous_chunk_data = last_id_in_chunk_data\n","        \n","        #calculate NA aggregates\n","        x_na_aggregate = chunk.groupby(\"customer_ID\")[self.numeric_columns].agg([missing_values, missing_last_value])\n","        x_na_aggregate.columns = ['_'.join(x) for x in x_na_aggregate.columns]\n","\n","        #Drop columns with too many na\n","        chunk.drop(self.columns_drop, axis=1, inplace=True)\n","        \n","        #fill NA\n","        numeric_means = self.aggregate_stats['mean'].to_dict()\n","        numeric_means = { x:numeric_means[x] for x in numeric_means if  x.startswith('R_') or x.startswith('D_')}\n","        print('filling na')\n","        chunk.fillna(numeric_means, inplace=True)\n","        chunk.fillna(self.categorical_mode, inplace=True)\n","        print('filling 0')\n","        chunk.fillna(0, inplace=True) #fill remaining with 0\n","\n","        #encode categorical values\n","        chunk.replace(self.categorical_encode, inplace=True)\n","        \n","        #set numeric datatype\n","        update_dtypes_numeric = {x: self.numeric_dtype for x in self.numeric_columns}\n","        chunk = chunk.astype(update_dtypes_numeric)\n","        \n","        update_dtypes_categorical = {x: np.int16 for x in self.categorical_columns}\n","        chunk = chunk.astype(update_dtypes_categorical)\n","                \n","        #clip outliers to max/min values\n","        n_deviations_limit = 10\n","        upper_limit = list(self.aggregate_stats['mean'] + self.aggregate_stats['std'] * n_deviations_limit)\n","        lower_limit = list(self.aggregate_stats['mean'] - self.aggregate_stats['std'] * n_deviations_limit)\n","        chunk[self.numeric_columns] = chunk[self.numeric_columns].clip(lower_limit,upper_limit, axis=1)\n","        \n","        #Remove noise from float columns ie turn float -> int\n","        for column_conversion in self.int_scaling: #column_conversion -> (column_name, add_value, multiply_value)\n","            chunk[column_conversion[0]] = ((chunk[column_conversion[0]] +  column_conversion[1])*column_conversion[2]).round(0).astype(np.int16)\n","#             if column_conversion[0] =='S_13':\n","#                 print(chunk[column_conversion[0]].unique())\n","\n","        \n","        #create aggregates for each customer_id numeric columns\n","        print('Doing Aggregates')\n","        x_aggregate = chunk.groupby(\"customer_ID\")[self.numeric_columns].agg(['first', 'mean', 'std', 'min', 'max', 'last', lm_diff])\n","        x_aggregate.columns = ['_'.join(x) for x in x_aggregate.columns]\n","        #fill std columns with 0 incase of only one value as it results in nan\n","        std_fill_na_columns = {x:0 for x in x_aggregate.columns if '_std' in x}\n","        x_aggregate.fillna(std_fill_na_columns, inplace=True)\n","        \n","        #create aggregates for each customer_id categorical columns\n","        x_aggregate_category = chunk.groupby(\"customer_ID\")[self.categorical_columns].agg(['first', 'last']) #,'nunique''count',  removed because dont seem to add much value\n","        x_aggregate_category.columns = ['_'.join(x) for x in x_aggregate_category.columns]\n","        \n","        #Get the number of months a record has been active\n","        chunk['S_2'] = pd.to_datetime(chunk['S_2'])\n","        x_date = chunk.groupby(\"customer_ID\")['S_2'].agg(['first','last'])\n","        x_aggregate['months'] = (x_date['last'].dt.year  - x_date['first'].dt.year)*12 + (x_date['last'].dt.month  - x_date['first'].dt.month)+1\n","        \n","        del chunk, x_date\n","        gc.collect()\n","\n","        #Features for how metrics have changed over time \n","        for column in  x_aggregate:\n","            if 'first' in column:\n","                column_first = column\n","                column = column.replace('_first', '')\n","                column_last = column +'_last'\n","\n","                x_aggregate[column+'_change_sub'] = (x_aggregate[column_first] - x_aggregate[column_last])/ x_aggregate['months']\n","                x_aggregate[column+'_change_div'] = ((x_aggregate[column_first] / x_aggregate[column_last])/ x_aggregate['months']).replace([np.inf, -np.inf], np.nan).fillna(0)\n","        \n","        \n","        # #create ratio features between payment and spending\n","        # payment_features = [col.replace('_first', '') for col in x_aggregate if col.startswith('P_') and col.endswith('_first')]\n","        # spend_features = [col.replace('_first', '') for col in x_aggregate if col.startswith('S_') and col.endswith('_first')]\n","        \n","        # column_suffix = ['_mean', '_last']\n","        # for p_column in payment_features:\n","        #     for s_column in spend_features:\n","        #         column_name = f'{p_column}_{s_column}'\n","        #         for suffix in column_suffix:\n","        #             x_aggregate[column_name + '_ratio' + suffix] = (x_aggregate[p_column+suffix] / x_aggregate[s_column+suffix]).replace([np.inf, -np.inf], np.nan).fillna(0)\n","        #             x_aggregate[column_name + '_sub' + suffix] = (x_aggregate[p_column+suffix] - x_aggregate[s_column+suffix])\n","                \n","\n","        #create combination features\n","        features = ['B_3','B_1','B_37','B_9','B_2','B_7','B_18','D_48','D_44','D_39','P_2','P_3','P_4','R_1','R_2','R_3','S_3','S_23','S_7']\n","        column_suffix = '_last'\n","        for p_column in features:\n","            #print(p_column)\n","            for s_column in features:\n","                column_name = f'{p_column}_{s_column}'\n","                x_aggregate[column_name + '_multi' + column_suffix] = (x_aggregate[p_column+column_suffix] * x_aggregate[s_column+column_suffix])\n","                if p_column != s_column:\n","                    x_aggregate[column_name + '_ratio' + column_suffix] = (x_aggregate[p_column+column_suffix] / x_aggregate[s_column+column_suffix]).replace([np.inf, -np.inf], np.nan).fillna(0)\n","                    x_aggregate[column_name + '_sub' + column_suffix] = (x_aggregate[p_column+column_suffix] - x_aggregate[s_column+column_suffix])\n","\n","\n","        update_cols = {x: np.float32 for x in x_aggregate.select_dtypes(include=[float]).columns}\n","        x_aggregate = x_aggregate.astype(update_cols)\n","        \n","        #feature for has the category change over time\n","        for column in  x_aggregate_category:\n","            if 'first' in column:\n","                column_first = column\n","                column = column.replace('_first', '')\n","                column_last = column +'_last'\n","\n","                x_aggregate_category[column+'_change'] = (x_aggregate_category[column_first] == x_aggregate_category[column_last]).astype(np.int16)\n","                \n","        chunk = pd.concat([x_aggregate, x_aggregate_category, x_na_aggregate], axis=1)\n","        \n","        #drop first aggregate\n","        columns_drop = [x for x in chunk.columns if '_first' in x]\n","        chunk.drop(columns_drop, axis=1, inplace=True)\n","\n","        return chunk\n","    \n","    def print_memory_usage(self, data, metric='MB', label=''):\n","        metric_mapping = {'B':0, 'KB': 2, 'MB': 2, 'GB': 3}\n","        multiplier= metric_mapping[metric]\n","        memory = data.memory_usage().sum() / (1024**multiplier)\n","        print(f'Memory usage {label}: {memory:.2f}{metric}')\n","\n","    \n","    def compress(self, data):\n","        INT8_MIN    = np.iinfo(np.int8).min\n","        INT8_MAX    = np.iinfo(np.int8).max\n","        INT16_MIN   = np.iinfo(np.int16).min\n","        INT16_MAX   = np.iinfo(np.int16).max\n","        INT32_MIN   = np.iinfo(np.int32).min\n","        INT32_MAX   = np.iinfo(np.int32).max\n","\n","        FLOAT16_MIN = np.finfo(np.float16).min\n","        FLOAT16_MAX = np.finfo(np.float16).max\n","        FLOAT32_MIN = np.finfo(np.float32).min\n","        FLOAT32_MAX = np.finfo(np.float32).max\n","        column_dtypes = {}\n","        for col in data.columns:\n","            col_dtype = data[col][:100].dtype\n","\n","            if col_dtype != 'object':\n","                col_series = data[col]\n","                col_min = col_series.min()\n","                col_max = col_series.max()\n","\n","                if col_dtype == 'float64':\n","#                     if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n","#                         column_dtypes[col] = np.float16\n","#                     elif\n","                    if (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n","                        column_dtypes[col] = np.float32\n","                    else:\n","                        pass\n","\n","\n","                if col_dtype == 'int64':\n","                    if (col_min > INT8_MIN/2) and (col_max < INT8_MAX/2):\n","                        column_dtypes[col] = np.int8\n","                    elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n","                        column_dtypes[col] = np.int16\n","                    elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n","                        column_dtypes[col] = np.int32\n","                    else:\n","                        pass\n","        return column_dtypes\n","    \n","    \n"]},{"cell_type":"markdown","metadata":{},"source":["## Import Data in batches"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T03:05:43.257936Z","iopub.status.busy":"2022-07-28T03:05:43.257546Z","iopub.status.idle":"2022-07-28T03:05:43.268791Z","shell.execute_reply":"2022-07-28T03:05:43.267672Z","shell.execute_reply.started":"2022-07-28T03:05:43.257900Z"},"trusted":true},"outputs":[],"source":["# x_input = a_amex_helper.read_data(filename, first_chunk_only=True)\n","# y_input = pd.read_csv(filename_label)\n","# y_input = y_input.loc[x_input.index, :]\n","# gc.collect()"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:32:28.331627Z","iopub.status.busy":"2022-07-28T04:32:28.331230Z","iopub.status.idle":"2022-07-28T04:32:28.344076Z","shell.execute_reply":"2022-07-28T04:32:28.342833Z","shell.execute_reply.started":"2022-07-28T04:32:28.331595Z"},"trusted":true},"outputs":[],"source":["with open('helper_object.pickle', 'rb') as handle:\n","    a_amex_helper = pickle.load(handle)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:32:29.921207Z","iopub.status.busy":"2022-07-28T04:32:29.920415Z","iopub.status.idle":"2022-07-28T04:32:56.234203Z","shell.execute_reply":"2022-07-28T04:32:56.233154Z","shell.execute_reply.started":"2022-07-28T04:32:29.921148Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory used Process: 0.2GB Memory used Total: 18.5GB\n","Memory used Process: 8.0GB Memory used Total: 26.3GB\n"]},{"data":{"text/plain":["8.025642395019531"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["memory_usage(\"GB\",1)\n","x_input = pd.read_parquet('x_input.parquet')\n","y_input = pd.read_parquet('y_input.parquet')\n","y_input = y_input['target']\n","gc.collect()\n","memory_usage(\"GB\",1)"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling Functions"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:56:19.093299Z","iopub.status.busy":"2022-07-28T04:56:19.092897Z","iopub.status.idle":"2022-07-28T04:56:19.104772Z","shell.execute_reply":"2022-07-28T04:56:19.103849Z","shell.execute_reply.started":"2022-07-28T04:56:19.093268Z"},"trusted":true},"outputs":[],"source":["def custom_metric(actual, preds):\n","    return 1 - amex_metric_numpy(actual,preds)\n","\n","def custom_metric_lgbm(actual, preds):\n","    return 'amex_metric', amex_metric_numpy(actual,preds), True\n","\n","def save_models(models, name):\n","    for i, model in enumerate(models):\n","        print(f'saving model_{name}_{i}')\n","        joblib.dump(model, model_save_path+f'model_{name}_fold_{i}')\n","\n","def kfold_training(model, x_input, y_input, print_messages=True, fit_params ={},  splits= 5, first_fold_only=False):\n","    skf = StratifiedKFold(n_splits=splits, random_state=random_state, shuffle=True)\n","    models = []\n","    scores = []\n","    if print_messages:\n","        print('starting cross validation training')\n","    for fold, (train_index, test_index)in enumerate(skf.split(x_input, y_input)):\n","        if print_messages:\n","            print('*'*100)\n","            print(' '*20 + f'Fold: {fold}' )\n","            print('*'*100)\n","        x_train, y_train = x_input.iloc[train_index], y_input.iloc[train_index]\n","        x_test, y_test = x_input.iloc[test_index], y_input.iloc[test_index]\n","        \n","        \n","        model_iter = clone(model)\n","        model_iter.fit(x_train, y_train, eval_set=[(x_test, y_test)], **fit_params)\n","        preds = model_iter.predict_proba(x_test)[:,1]\n","\n","        score = amex_metric_numpy(y_test.to_numpy(), preds)\n","        scores.append(score)\n","        models.append(model_iter)\n","        joblib.dump(model_iter, model_save_path+f'model_temp_training_fold_{fold}')\n","        \n","        if print_messages:\n","            print(f'fold: {fold}')\n","            eval_model_classification_report( x_test, y_test, model_iter)\n","            print('\\n'*3)\n","\n","        \n","        if first_fold_only:\n","            break\n","    \n","    overall_score = np.array(scores).mean()\n","    \n","    \n","    if print_messages:\n","        print(f'Cross Validation score: {overall_score}')\n","    \n","    \n","    \n","    return overall_score, models\n","\n","def get_feature_importance_lgbm(model):\n","    importance_df = (\n","        pd.DataFrame({\n","            'feature_name': model.booster_.feature_name(),\n","            'importance_gain': model.booster_.feature_importance(importance_type='gain'),\n","            'importance_split': model.booster_.feature_importance(importance_type='split'),\n","        })\n","        .sort_values('importance_gain', ascending=False)\n","        .reset_index(drop=True)\n","    )\n","    return importance_df\n","\n","#for testing kfold training\n","# model = XGBClassifier()\n","# fit_params ={'verbose':False}\n","# kfold_training(model,x_input.iloc[:1000],y_input.iloc[:1000], fit_params=fit_params)"]},{"cell_type":"markdown","metadata":{},"source":["## **XGB Modeling**"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T03:04:26.585431Z","iopub.status.busy":"2022-07-28T03:04:26.584921Z","iopub.status.idle":"2022-07-28T03:04:26.595536Z","shell.execute_reply":"2022-07-28T03:04:26.594386Z","shell.execute_reply.started":"2022-07-28T03:04:26.585391Z"},"trusted":true},"outputs":[],"source":["\n","\n","# def objective_tune(trial: Trial, x_train, y_train, x_test, y_test) -> float:\n","\n","#     param = {\n","#                 \"n_estimators\" : trial.suggest_int('n_estimators', 100, 2600, log=True),\n","#                 'tree_method': 'gpu_hist',\n","#                 'max_delta_step' : trial.suggest_int('max_delta_step', 1, 10),\n","#                 'max_depth':trial.suggest_int('max_depth', 3, 10),\n","#                 'min_child_weight':trial.suggest_int('min_child_weight', 0, 5),\n","#                 'gamma':trial.suggest_discrete_uniform('gamma', 0, 5, 0.5),\n","#                 'learning_rate':trial.suggest_loguniform('learning_rate',0.05,0.3),\n","#                 'colsample_bytree':trial.suggest_discrete_uniform('colsample_bytree',0.5,0.9,.1),\n","#                 'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n","#                 'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n","#                 'eta':trial.suggest_loguniform('eta', 1e-8, 1.0),\n","#                 'n_jobs' : -1,\n","#                 'eval_metric' : custom_metric,\n","#                 'objective' : 'binary:logistic',\n","#                 'scale_pos_weight': trial.suggest_int('scale_pos_weight', 1, 5),\n","#                 'random_state': 2022\n","                 \n","#             }\n","#     model = XGBClassifier(**param)\n","\n","#     model.fit(x_train,y_train)\n","    \n","#     preds = model.predict_proba(x_test)[:,1]\n","#     actual = np.array(y_test).squeeze()\n","    \n","#     return a_amex_helper.amex_metric_numpy(actual,preds)\n","\n","\n","# print('Parameter Tuning is starting now...')\n","# study = optuna.create_study(direction='maximize',sampler=TPESampler(),)\n","# study.optimize(lambda trial : objective_tune(trial,x_train, y_train, x_test, y_test),n_trials= 50)\n","# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:33:29.296763Z","iopub.status.busy":"2022-07-28T04:33:29.296388Z","iopub.status.idle":"2022-07-28T04:33:29.306287Z","shell.execute_reply":"2022-07-28T04:33:29.305301Z","shell.execute_reply.started":"2022-07-28T04:33:29.296731Z"},"trusted":true},"outputs":[],"source":["fixed_params = {\n","             #'n_jobs' : -1,\n","             #'objective':'aucpr',\n","             #'eval_metric' : 'aucpr',\n","             'eval_metric' : custom_metric,\n","             'early_stopping_rounds' : 500,\n","             'objective' : 'binary:logistic',\n","            'random_state': random_state,\n","            'tree_method': 'gpu_hist',\n","            'predictor': 'gpu_predictor',\n","            }\n","\n","search_params = {\n","    'n_estimators': 4000, \n","    'max_delta_step': 4, \n","    'max_depth': 4, \n","    'min_child_weight': 3, \n","    'gamma': 3.0, \n","    'learning_rate': 0.059, \n","    'colsample_bytree': 0.5, \n","    'lambda': 0.332, \n","    'alpha': 0.018, \n","    'scale_pos_weight': 1}\n","\n","# params = {\n","#         'max_depth': 7,\n","#         'eta': 0.03,\n","#         'subsample': 0.88,\n","#         'colsample_bytree': 0.5,\n","#         'gamma': 1.5,\n","#         'min_child_weight': 8,\n","#         'lambda': 70,\n","#     }\n","#model = XGBClassifier(**fixed_params, **search_params)\n","\n","\n","# model.fit(x_train, y_train, eval_set=eval_set, verbose=True)\n","# eval_model_classification_report( x_test, y_test, model)\n","# joblib.dump(model, model_save_path+'xgb')\n","\n","\n","# model = XGBClassifier(**fixed_params, **search_params)\n","# fit_params ={'verbose':False}\n","# score, models = kfold_training(model,x_input,y_input, fit_params=fit_params)\n","# save_models(models, 'xgb')"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:33:32.952160Z","iopub.status.busy":"2022-07-28T04:33:32.951801Z","iopub.status.idle":"2022-07-28T04:37:01.679810Z","shell.execute_reply":"2022-07-28T04:37:01.678790Z","shell.execute_reply.started":"2022-07-28T04:33:32.952130Z"},"trusted":true},"outputs":[],"source":["# model = XGBClassifier(**fixed_params)\n","# fit_params ={'verbose':False}\n","# score, models = kfold_training(model,x_input,y_input, fit_params=fit_params)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:14:13.347730Z","iopub.status.idle":"2022-07-27T06:14:13.348782Z","shell.execute_reply":"2022-07-27T06:14:13.348539Z","shell.execute_reply.started":"2022-07-27T06:14:13.348515Z"},"trusted":true},"outputs":[],"source":["# frames = []\n","# for model in models:\n","#     gain = model.get_booster().get_score(importance_type='gain')\n","#     gain = pd.DataFrame.from_dict(gain, orient='index', columns=['gain'])\n","#     frames.append(gain)\n","# importance = pd.concat(frames)\n","# importance = importance.groupby(level=0).sum()/len(models)\n","# importance = importance.sort_values('gain',ascending=False)\n","# #importance.agg(['min','max','mean','std'])\n","# mean = importance.mean()[0]\n","# print(mean)\n","# model_select = importance[importance['gain']>mean]\n","# top_features= list(model_select.reset_index()['index'])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:14:13.350415Z","iopub.status.idle":"2022-07-27T06:14:13.351434Z","shell.execute_reply":"2022-07-27T06:14:13.351192Z","shell.execute_reply.started":"2022-07-27T06:14:13.351168Z"},"trusted":true},"outputs":[],"source":["# model = XGBClassifier(**fixed_params, **search_params)\n","# fit_params ={'verbose':False}\n","# score, models = kfold_training(model, x_input[top_features], y_input, fit_params=fit_params)\n","# save_models(models, 'xgb')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.status.busy":"2022-07-27T06:14:13.359225Z","iopub.status.idle":"2022-07-27T06:14:13.360071Z","shell.execute_reply":"2022-07-27T06:14:13.359851Z","shell.execute_reply.started":"2022-07-27T06:14:13.359820Z"},"trusted":true},"outputs":[],"source":["#Dart Testing\n","# param = {\n","#     'booster': 'dart',\n","#     'tree_method': 'gpu_hist',\n","#     'n_estimators': 1056,\n","#     'max_depth': 5, \n","#     'learning_rate': 0.1,\n","#     'objective': 'binary:logistic',\n","#     'sample_type': 'uniform',\n","#     'normalize_type': 'tree',\n","#     'rate_drop': 0.1,\n","#     'skip_drop': 0.5}\n","\n","# fit_params ={'verbose':False}\n","# model = XGBClassifier(**param)\n","# score, models = kfold_training(model,x_input,y_input, fit_params=fit_params)\n","#eval_model_classification_report(x_input,y_input, models)"]},{"cell_type":"markdown","metadata":{},"source":["## LGB Modelling"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:37:41.475292Z","iopub.status.busy":"2022-07-28T04:37:41.474578Z","iopub.status.idle":"2022-07-28T04:41:05.122259Z","shell.execute_reply":"2022-07-28T04:41:05.120603Z","shell.execute_reply.started":"2022-07-28T04:37:41.475257Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Validation Results - Amex metric: 0.786 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.93      0.93     74282\n","     Default       0.81      0.81      0.81     25717\n","\n","    accuracy                           0.90     99999\n","   macro avg       0.87      0.87      0.87     99999\n","weighted avg       0.90      0.90      0.90     99999\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     69430     4852\n","true:no       4962    20755\n"]}],"source":["model = LGBMClassifier(n_estimators = 500)\n","model.fit(x_input.iloc[:-100000],y_input.iloc[:-100000])\n","eval_model_classification_report( x_input.iloc[-99999:], y_input.iloc[-99999:], model)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:57:23.337507Z","iopub.status.busy":"2022-07-28T04:57:23.336622Z","iopub.status.idle":"2022-07-28T04:57:23.359990Z","shell.execute_reply":"2022-07-28T04:57:23.359093Z","shell.execute_reply.started":"2022-07-28T04:57:23.337456Z"},"trusted":true},"outputs":[],"source":["\n","# compute importances\n","def get_feature_importance_lgbm(model):\n","    importance_df = (\n","        pd.DataFrame({\n","            'feature_name': model.booster_.feature_name(),\n","            'importance_gain': model.booster_.feature_importance(importance_type='gain'),\n","            'importance_split': model.booster_.feature_importance(importance_type='split'),\n","        })\n","        .sort_values('importance_gain', ascending=False)\n","        .reset_index(drop=True)\n","    )\n","    return importance_df\n","\n","\n","def get_feature_importance_lgbm_models(models):\n","    feature_importance = []\n","    for i, model in enumerate(models):\n","        feature_importance_fold = get_feature_importance_lgbm(model)\n","        feature_importance_fold.to_csv(f'Tableau\\\\feature_importance_{i}.csv')\n","        feature_importance.append(feature_importance_fold)\n","    feature_importance = pd.concat(feature_importance)\n","    feature_importance = feature_importance.groupby('feature_name').sum().sort_values('importance_gain', ascending=False)\n","    feature_importance['folds'] = len(models)\n","    feature_importance['max_gain'] = feature_importance['importance_gain'].max()\n","    feature_importance['total_gain'] = feature_importance['importance_gain'].sum()\n","    feature_importance['percent_of_max'] = feature_importance['importance_gain'] / feature_importance['max_gain']\n","    feature_importance['percent_of_total'] = feature_importance['importance_gain'] / feature_importance['total_gain']\n","    feature_importance['percent_of_total_cumulative'] = feature_importance['percent_of_total'].cumsum()\n","    \n","    return feature_importance"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance_gain</th>\n","      <th>importance_split</th>\n","      <th>folds</th>\n","      <th>max_gain</th>\n","      <th>total_gain</th>\n","      <th>percent_of_max</th>\n","      <th>percent_of_total</th>\n","      <th>percent_of_total_cumulative</th>\n","    </tr>\n","    <tr>\n","      <th>feature_name</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>B_1_D_44_multi_last</th>\n","      <td>269946.110946</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>1.000000</td>\n","      <td>0.175185</td>\n","      <td>0.175185</td>\n","    </tr>\n","    <tr>\n","      <th>D_44_B_2_sub_last</th>\n","      <td>115787.386146</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.428928</td>\n","      <td>0.075142</td>\n","      <td>0.250326</td>\n","    </tr>\n","    <tr>\n","      <th>D_44_B_2_ratio_last</th>\n","      <td>103434.938438</td>\n","      <td>12</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.383169</td>\n","      <td>0.067125</td>\n","      <td>0.317452</td>\n","    </tr>\n","    <tr>\n","      <th>B_1_P_2_sub_last</th>\n","      <td>95621.583405</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.354225</td>\n","      <td>0.062055</td>\n","      <td>0.379506</td>\n","    </tr>\n","    <tr>\n","      <th>P_2_B_1_sub_last</th>\n","      <td>59811.817690</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.221569</td>\n","      <td>0.038816</td>\n","      <td>0.418322</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>P_3_B_18_multi_last</th>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>P_3_B_1_multi_last</th>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>P_3_B_1_sub_last</th>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>P_3_B_2_multi_last</th>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>months</th>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>269946.110946</td>\n","      <td>1.540923e+06</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2516 rows Ã— 8 columns</p>\n","</div>"],"text/plain":["                     importance_gain  importance_split  folds       max_gain  \\\n","feature_name                                                                   \n","B_1_D_44_multi_last    269946.110946                12      1  269946.110946   \n","D_44_B_2_sub_last      115787.386146                 8      1  269946.110946   \n","D_44_B_2_ratio_last    103434.938438                12      1  269946.110946   \n","B_1_P_2_sub_last        95621.583405                 5      1  269946.110946   \n","P_2_B_1_sub_last        59811.817690                 8      1  269946.110946   \n","...                              ...               ...    ...            ...   \n","P_3_B_18_multi_last         0.000000                 0      1  269946.110946   \n","P_3_B_1_multi_last          0.000000                 0      1  269946.110946   \n","P_3_B_1_sub_last            0.000000                 0      1  269946.110946   \n","P_3_B_2_multi_last          0.000000                 0      1  269946.110946   \n","months                      0.000000                 0      1  269946.110946   \n","\n","                       total_gain  percent_of_max  percent_of_total  \\\n","feature_name                                                          \n","B_1_D_44_multi_last  1.540923e+06        1.000000          0.175185   \n","D_44_B_2_sub_last    1.540923e+06        0.428928          0.075142   \n","D_44_B_2_ratio_last  1.540923e+06        0.383169          0.067125   \n","B_1_P_2_sub_last     1.540923e+06        0.354225          0.062055   \n","P_2_B_1_sub_last     1.540923e+06        0.221569          0.038816   \n","...                           ...             ...               ...   \n","P_3_B_18_multi_last  1.540923e+06        0.000000          0.000000   \n","P_3_B_1_multi_last   1.540923e+06        0.000000          0.000000   \n","P_3_B_1_sub_last     1.540923e+06        0.000000          0.000000   \n","P_3_B_2_multi_last   1.540923e+06        0.000000          0.000000   \n","months               1.540923e+06        0.000000          0.000000   \n","\n","                     percent_of_total_cumulative  \n","feature_name                                      \n","B_1_D_44_multi_last                     0.175185  \n","D_44_B_2_sub_last                       0.250326  \n","D_44_B_2_ratio_last                     0.317452  \n","B_1_P_2_sub_last                        0.379506  \n","P_2_B_1_sub_last                        0.418322  \n","...                                          ...  \n","P_3_B_18_multi_last                     1.000000  \n","P_3_B_1_multi_last                      1.000000  \n","P_3_B_1_sub_last                        1.000000  \n","P_3_B_2_multi_last                      1.000000  \n","months                                  1.000000  \n","\n","[2516 rows x 8 columns]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["models_single = [model]\n","feature_importance = get_feature_importance_lgbm_models(models_single)\n","feature_importance"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fixed_params ={\n","    'objective': 'binary',\n","    'metric': 'binary_logloss',\n","    'boosting': 'dart',\n","    #'device': 'gpu',\n","    'histogram_pool_size' : 2000\n","}\n","\n","search_params ={\n","    'num_leaves': 100,\n","    'learning_rate': 0.01,\n","    'colsample_bytree': 0.20,\n","    'subsample_freq': 10,\n","    'subsample': 0.50,\n","    'reg_lambda': 2,\n","    'min_child_samples': 40,\n","    'n_estimators': 500\n","}\n","\n","cut_offs = [0.8, 0.85, 0.9, 9.5]\n","scores =[]\n","for cut_off in cut_offs:\n","    features = list(feature_importance[feature_importance['percent_of_total_cumulative']<cut_off].reset_index()['feature_name'])\n","\n","    model = LGBMClassifier(**fixed_params, **search_params)\n","    fit_params = {'callbacks': [log_evaluation(period=1000)], 'eval_metric': custom_metric_lgbm}\n","\n","    score, _ = kfold_training(model, x_input[features], y_input, fit_params=fit_params)\n","    scores.append(score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["[0.7623336149608031,\n"," 0.7624564944340936,\n"," 0.7626718636848588,\n"," 0.7624094427285708]"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["scores\n","#0.7625"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-27T06:20:13.659288Z","iopub.status.busy":"2022-07-27T06:20:13.658274Z","iopub.status.idle":"2022-07-27T06:20:13.671874Z","shell.execute_reply":"2022-07-27T06:20:13.670770Z","shell.execute_reply.started":"2022-07-27T06:20:13.659248Z"},"trusted":true},"outputs":[],"source":["#I think the memory consuming here is used for the histogram cache. it needs about num_leaves * 20Bytes * num_features * num_bins.\n","\n","def objective_tune(trial: Trial) -> float:\n","    search_params = { \n","        'learning_rate' : trial.suggest_loguniform('learning_rate', .01, 0.3),\n","        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),\n","        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),\n","        'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n","        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 100, 10000, step=100),\n","        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 15),\n","        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.1, 1.0),\n","        'subsample': trial.suggest_uniform('subsample', 0.1, 1.0),\n","        'subsample_freq': trial.suggest_int('subsample_freq', 0, 15),\n","        'n_estimators': trial.suggest_int('n_estimators', 100, 10000, log=True),\n","    }\n","\n","    fixed_params={\n","        'objective': 'binary',\n","        'metric': 'None',\n","        #'eval_metric': custom_metric,\n","        'boosting_type' : 'dart',\n","        'force_row_wise' : True,\n","        #'device': 'gpu',\n","        'max_bin': 255,\n","        'random_state' : random_state,\n","        'extra_trees' : True,\n","        'feature_pre_filter': False,\n","        'histogram_pool_size' : 2000,\n","        'early_stopping_round': 5\n","    }\n","    model = LGBMClassifier(**fixed_params, **search_params)\n","\n","    y_train_numpy = np.array(y_train).squeeze()\n","    y_test_numpy = np.array(y_test).squeeze()\n","    \n","    model.fit(x_train,y_train_numpy, eval_set=[(x_test, y_test_numpy)], eval_metric=custom_metric_lgbm, callbacks =[log_evaluation(period=0)])\n","    \n","    preds = model.predict_proba(x_test)[:,1]\n","    \n","    gc.collect()\n","    return amex_metric_numpy(y_test_numpy,preds)\n","\n","\n","# print('Parameter Tuning is starting now...')\n","# study = optuna.create_study(direction='maximize',sampler=TPESampler())\n","# study.optimize(lambda trial : objective_tune(trial),n_trials= 70)\n","# print('Best trial: score {},\\nparams {}'.format(study.best_trial.value,study.best_trial.params))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:57:52.258357Z","iopub.status.busy":"2022-07-28T04:57:52.257690Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["starting cross validation training\n","****************************************************************************************************\n","                    Fold: 0\n","****************************************************************************************************\n","[1000]\tvalid_0's binary_logloss: 0.248942\tvalid_0's amex_metric: 0.778506\n","[2000]\tvalid_0's binary_logloss: 0.226258\tvalid_0's amex_metric: 0.787849\n","[3000]\tvalid_0's binary_logloss: 0.220707\tvalid_0's amex_metric: 0.792286\n","[4000]\tvalid_0's binary_logloss: 0.21847\tvalid_0's amex_metric: 0.793615\n","[5000]\tvalid_0's binary_logloss: 0.217641\tvalid_0's amex_metric: 0.79406\n","[6000]\tvalid_0's binary_logloss: 0.21692\tvalid_0's amex_metric: 0.795695\n","[7000]\tvalid_0's binary_logloss: 0.216486\tvalid_0's amex_metric: 0.795777\n","[8000]\tvalid_0's binary_logloss: 0.216227\tvalid_0's amex_metric: 0.796283\n","[9000]\tvalid_0's binary_logloss: 0.216098\tvalid_0's amex_metric: 0.7965\n","[10000]\tvalid_0's binary_logloss: 0.216019\tvalid_0's amex_metric: 0.796267\n","fold: 0\n","Validation Results - Amex metric: 0.796 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.94      0.94     68017\n","     Default       0.82      0.81      0.81     23766\n","\n","    accuracy                           0.90     91783\n","   macro avg       0.88      0.87      0.87     91783\n","weighted avg       0.90      0.90      0.90     91783\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     63816     4201\n","true:no       4582    19184\n","\n","\n","\n","\n","****************************************************************************************************\n","                    Fold: 1\n","****************************************************************************************************\n","[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n","[1000]\tvalid_0's binary_logloss: 0.250338\tvalid_0's amex_metric: 0.76888\n","[2000]\tvalid_0's binary_logloss: 0.228521\tvalid_0's amex_metric: 0.778123\n","[3000]\tvalid_0's binary_logloss: 0.223431\tvalid_0's amex_metric: 0.78349\n","[4000]\tvalid_0's binary_logloss: 0.221455\tvalid_0's amex_metric: 0.786586\n","[5000]\tvalid_0's binary_logloss: 0.220746\tvalid_0's amex_metric: 0.786633\n","[6000]\tvalid_0's binary_logloss: 0.22021\tvalid_0's amex_metric: 0.787825\n","[7000]\tvalid_0's binary_logloss: 0.219977\tvalid_0's amex_metric: 0.787871\n","[8000]\tvalid_0's binary_logloss: 0.219737\tvalid_0's amex_metric: 0.788352\n","[9000]\tvalid_0's binary_logloss: 0.219669\tvalid_0's amex_metric: 0.787812\n","[10000]\tvalid_0's binary_logloss: 0.219725\tvalid_0's amex_metric: 0.788057\n","fold: 1\n","Validation Results - Amex metric: 0.788 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.94      0.93     68017\n","     Default       0.82      0.80      0.81     23766\n","\n","    accuracy                           0.90     91783\n","   macro avg       0.87      0.87      0.87     91783\n","weighted avg       0.90      0.90      0.90     91783\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     63738     4279\n","true:no       4646    19120\n","\n","\n","\n","\n","****************************************************************************************************\n","                    Fold: 2\n","****************************************************************************************************\n","[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n","[1000]\tvalid_0's binary_logloss: 0.251013\tvalid_0's amex_metric: 0.774321\n","[2000]\tvalid_0's binary_logloss: 0.228759\tvalid_0's amex_metric: 0.784993\n","[3000]\tvalid_0's binary_logloss: 0.223267\tvalid_0's amex_metric: 0.788889\n","[4000]\tvalid_0's binary_logloss: 0.221061\tvalid_0's amex_metric: 0.791456\n","[5000]\tvalid_0's binary_logloss: 0.220309\tvalid_0's amex_metric: 0.792099\n","[6000]\tvalid_0's binary_logloss: 0.219693\tvalid_0's amex_metric: 0.79305\n","[7000]\tvalid_0's binary_logloss: 0.21943\tvalid_0's amex_metric: 0.792851\n","[8000]\tvalid_0's binary_logloss: 0.21924\tvalid_0's amex_metric: 0.791894\n","[9000]\tvalid_0's binary_logloss: 0.219158\tvalid_0's amex_metric: 0.791827\n","[10000]\tvalid_0's binary_logloss: 0.219122\tvalid_0's amex_metric: 0.792389\n","fold: 2\n","Validation Results - Amex metric: 0.793 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.94      0.94     68017\n","     Default       0.82      0.80      0.81     23766\n","\n","    accuracy                           0.90     91783\n","   macro avg       0.88      0.87      0.87     91783\n","weighted avg       0.90      0.90      0.90     91783\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     63853     4164\n","true:no       4703    19063\n","\n","\n","\n","\n","****************************************************************************************************\n","                    Fold: 3\n","****************************************************************************************************\n","[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n","[1000]\tvalid_0's binary_logloss: 0.251772\tvalid_0's amex_metric: 0.770924\n","[2000]\tvalid_0's binary_logloss: 0.229603\tvalid_0's amex_metric: 0.78165\n","[3000]\tvalid_0's binary_logloss: 0.224277\tvalid_0's amex_metric: 0.785378\n","[4000]\tvalid_0's binary_logloss: 0.222019\tvalid_0's amex_metric: 0.786047\n","[5000]\tvalid_0's binary_logloss: 0.221217\tvalid_0's amex_metric: 0.787262\n","[6000]\tvalid_0's binary_logloss: 0.220613\tvalid_0's amex_metric: 0.787621\n","[7000]\tvalid_0's binary_logloss: 0.220287\tvalid_0's amex_metric: 0.788565\n","[8000]\tvalid_0's binary_logloss: 0.220037\tvalid_0's amex_metric: 0.789186\n","[9000]\tvalid_0's binary_logloss: 0.219912\tvalid_0's amex_metric: 0.789208\n","[10000]\tvalid_0's binary_logloss: 0.2199\tvalid_0's amex_metric: 0.789516\n","fold: 3\n","Validation Results - Amex metric: 0.789 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.94      0.93     68017\n","     Default       0.82      0.81      0.81     23765\n","\n","    accuracy                           0.90     91782\n","   macro avg       0.87      0.87      0.87     91782\n","weighted avg       0.90      0.90      0.90     91782\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     63716     4301\n","true:no       4608    19157\n","\n","\n","\n","\n","****************************************************************************************************\n","                    Fold: 4\n","****************************************************************************************************\n","[LightGBM] [Warning] boosting is set=dart, boosting_type=gbdt will be ignored. Current value: boosting=dart\n","[1000]\tvalid_0's binary_logloss: 0.250132\tvalid_0's amex_metric: 0.773974\n","[2000]\tvalid_0's binary_logloss: 0.22777\tvalid_0's amex_metric: 0.783188\n","[3000]\tvalid_0's binary_logloss: 0.222322\tvalid_0's amex_metric: 0.787445\n","[4000]\tvalid_0's binary_logloss: 0.219991\tvalid_0's amex_metric: 0.78952\n","[5000]\tvalid_0's binary_logloss: 0.219143\tvalid_0's amex_metric: 0.791203\n","[6000]\tvalid_0's binary_logloss: 0.21851\tvalid_0's amex_metric: 0.791117\n","[7000]\tvalid_0's binary_logloss: 0.218123\tvalid_0's amex_metric: 0.791874\n","[8000]\tvalid_0's binary_logloss: 0.217871\tvalid_0's amex_metric: 0.792424\n","[9000]\tvalid_0's binary_logloss: 0.217708\tvalid_0's amex_metric: 0.792278\n","[10000]\tvalid_0's binary_logloss: 0.217693\tvalid_0's amex_metric: 0.792214\n","fold: 4\n","Validation Results - Amex metric: 0.792 \n","\n","Classification Report\n","\n","              precision    recall  f1-score   support\n","\n","  No Default       0.93      0.94      0.93     68017\n","     Default       0.81      0.81      0.81     23765\n","\n","    accuracy                           0.90     91782\n","   macro avg       0.87      0.87      0.87     91782\n","weighted avg       0.90      0.90      0.90     91782\n","\n","\n","\n","Confusion Matrix\n","\n","          pred:yes  pred:no\n","true:yes     63649     4368\n","true:no       4569    19196\n","\n","\n","\n","\n","Cross Validation score: 0.7917628953665466\n"]}],"source":["fixed_params ={\n","    'objective': 'binary',\n","    'metric': 'binary_logloss',\n","    'boosting': 'dart',\n","    #'device': 'gpu',\n","    'histogram_pool_size' : 2000\n","}\n","\n","search_params ={\n","    'num_leaves': 100,\n","    'learning_rate': 0.01,\n","    'colsample_bytree': 0.20,\n","    'subsample_freq': 10,\n","    'subsample': 0.50,\n","    'reg_lambda': 2,\n","    'min_child_samples': 40,\n","    'n_estimators': 10500\n","}\n","\n","\n","\n","model = LGBMClassifier(**fixed_params, **search_params)\n","fit_params = {'callbacks': [log_evaluation(period=1000)], 'eval_metric': custom_metric_lgbm}\n","\n","score, models = kfold_training(model, x_input, y_input, fit_params=fit_params)\n","# model.fit(x_train,y_train, eval_set=[(x_test, y_test)], eval_metric=custom_metric_lgbm, callbacks =[log_evaluation(period=0)], verbose=10)\n","#eval_model_classification_report( x_input, y_input, model)\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["feature_importance = get_feature_importance_lgbm_models(models)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-07-24T23:35:50.572519Z","iopub.status.busy":"2022-07-24T23:35:50.571826Z","iopub.status.idle":"2022-07-24T23:35:50.784167Z","shell.execute_reply":"2022-07-24T23:35:50.782735Z","shell.execute_reply.started":"2022-07-24T23:35:50.572468Z"},"trusted":true},"outputs":[{"data":{"text/plain":["44247"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["del x_input, y_input\n","gc.collect()"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["saving model_lgb_10k_2_0\n","saving model_lgb_10k_2_1\n","saving model_lgb_10k_2_2\n","saving model_lgb_10k_2_3\n","saving model_lgb_10k_2_4\n"]}],"source":["save_models(models, 'lgb_10k_2')"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-07-27T07:04:38.079690Z","iopub.status.busy":"2022-07-27T07:04:38.078489Z","iopub.status.idle":"2022-07-27T07:22:53.323937Z","shell.execute_reply":"2022-07-27T07:22:53.322855Z","shell.execute_reply.started":"2022-07-27T07:04:38.079623Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading chunk: 1\n","filling na\n","filling 0\n","Doing Aggregates\n","Reading chunk: 2\n","filling na\n","filling 0\n","Doing Aggregates\n","Reading chunk: 3\n","filling na\n","filling 0\n","Doing Aggregates\n","Reading chunk: 4\n","filling na\n","filling 0\n","Doing Aggregates\n","Reading chunk: 5\n","filling na\n","filling 0\n","Doing Aggregates\n","Reading chunk: 6\n","filling na\n","filling 0\n","Doing Aggregates\n"]}],"source":["predictions = []\n","predictions_single = []\n","first_chunk_only= False\n","a_amex_helper.chunksize = 2000000\n","with pd.read_csv(filename_test, chunksize=a_amex_helper.chunksize) as reader:\n","    for i, chunk in enumerate(reader):\n","        print(f'Reading chunk: {i+1}')\n","        \n","        \n","        x_score = a_amex_helper.process_chunk(chunk)\n","        x_score = x_score.reset_index()\n","        output = pd.DataFrame(x_score['customer_ID'].copy())\n","        output_single = pd.DataFrame(x_score['customer_ID'].copy())\n","        x_score.drop('customer_ID', axis=1, inplace=True)\n","        \n","        \n","        #score each of models from cross validation\n","        best_iter = 0\n","        score = np.zeros(len(x_score))\n","        for i, model in enumerate(models): \n","            score += model.predict_proba(x_score)[:,1]\n","            if i == best_iter:\n","                output_single[\"prediction\"] = model.predict_proba(x_score)[:,1]\n","\n","        #weight each one equally\n","        score /= len(models)\n","        output[\"prediction\"] = score\n","       \n","        #x_score = column_transformer.transform(x_score)\n","        \n","        \n","        predictions.append(output)\n","        predictions_single.append(output_single)\n","\n","        if first_chunk_only:\n","            break\n","        \n","predictions_single=pd.concat(predictions_single).sort_index().reset_index(drop=True)        \n","predictions=pd.concat(predictions).sort_index().reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-27T06:04:04.180637Z","iopub.status.busy":"2022-07-27T06:04:04.179845Z","iopub.status.idle":"2022-07-27T06:04:04.189179Z","shell.execute_reply":"2022-07-27T06:04:04.187891Z","shell.execute_reply.started":"2022-07-27T06:04:04.180594Z"},"trusted":true},"outputs":[],"source":["#924621\n","len(predictions)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-07-27T07:23:39.780503Z","iopub.status.busy":"2022-07-27T07:23:39.780088Z","iopub.status.idle":"2022-07-27T07:23:44.090422Z","shell.execute_reply":"2022-07-27T07:23:44.089031Z","shell.execute_reply.started":"2022-07-27T07:23:39.780467Z"},"trusted":true},"outputs":[],"source":["predictions.to_csv(\"submission.csv\", index=False)\n","predictions_single.to_csv(\"submission_best_iter.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"}}},"nbformat":4,"nbformat_minor":4}
