{"cells":[{"cell_type":"code","execution_count":14,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-07-28T04:12:42.855824Z","iopub.status.busy":"2022-07-28T04:12:42.855268Z","iopub.status.idle":"2022-07-28T04:12:42.889060Z","shell.execute_reply":"2022-07-28T04:12:42.887831Z","shell.execute_reply.started":"2022-07-28T04:12:42.855710Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","#import scipy\n","import pandas as pd\n","from pandas.api.types import is_numeric_dtype\n","import gc\n","import os, psutil\n","import pickle\n","\n","from warnings import simplefilter\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","pd.set_option('mode.chained_assignment', None)\n","\n","base_dir =  'kaggle/input/amex-default-prediction/'\n","filename = base_dir + 'train_data.csv'\n","filename_label = base_dir + 'train_labels.csv'\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:12:42.915425Z","iopub.status.busy":"2022-07-28T04:12:42.914056Z","iopub.status.idle":"2022-07-28T04:12:42.923451Z","shell.execute_reply":"2022-07-28T04:12:42.922347Z","shell.execute_reply.started":"2022-07-28T04:12:42.915372Z"},"trusted":true},"outputs":[],"source":["def memory_usage(metric='MB', places=1,print_out=True):\n","    metric_mapping = {'B':0, 'KB': 2, 'MB': 2, 'GB': 3}\n","    multiplier= metric_mapping[metric]\n","    \n","    mem_used = psutil.Process(os.getpid()).memory_info().rss / 1024 ** multiplier\n","    mem_used_total = psutil.virtual_memory()[3] /1024**multiplier\n","    if print_out:\n","        print(f'Memory used Process: {mem_used:.{places}F}{metric} Memory used Total: {mem_used_total:.{places}F}{metric}')\n","\n","    return mem_used"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:12:43.125942Z","iopub.status.busy":"2022-07-28T04:12:43.125117Z","iopub.status.idle":"2022-07-28T04:12:43.204758Z","shell.execute_reply":"2022-07-28T04:12:43.203808Z","shell.execute_reply.started":"2022-07-28T04:12:43.125893Z"},"trusted":true},"outputs":[],"source":["#custom aggregate functions\n","def lm_diff(series):\n","    if len(series)>1:\n","        return series.iloc[-1] - series.iloc[-2]\n","    else:\n","        return 0\n","\n","def squared_mean(series):\n","    return (series**2).mean()\n","\n","def missing_values(series):\n","    return series.isna().sum()\n","\n","def missing_last_value(series):\n","    return series.isna().sum()\n","\n","def missing_last_value(series):\n","    return series.isna().iloc[-1].astype(int)\n","\n","class amex_helper():\n","      \n","    def __init__(self, chunksize= 500000):\n","        self.previous_chunk_data= None\n","        self.chunksize = chunksize\n","        self.key_columns = ['customer_ID','S_2']\n","        self.categorical_columns = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n","        self.non_numeric_columns = self.key_columns + self.categorical_columns\n","        \n","        self.numeric_dtype = np.float32\n","        \n","        #Values gotten from for converting to inshttps://www.kaggle.com/code/raddar/amex-data-int-types-train/notebook\n","        self.int_scaling = [\n","            ('B_4',0,78), ('B_16',0,12), ('B_19',0,1), ('B_20',0,17), ('B_22',0,2), ('B_31',0,1), ('B_32',0,1), ('B_33',0,1), ('B_41',0,1), \n","            ('D_39',0,34), ('D_44',0,8), ('D_49',0,71), ('D_51',0,3), ('D_59',48/5,48), ('D_65',0,38), ('D_70',0,4), \n","            ('D_72',0,3), ('D_74',0,14), ('D_75',0,15), ('D_78',0,2), ('D_79',0,2), ('D_80',0,5), ('D_81',0,1), ('D_82',0,2), \n","            ('D_83',0,1), ('D_84',0,2), ('D_86',0,1), ('D_87',0,1), ('D_89',0,9), ('D_91',0,2), ('D_92',0,1), ('D_93',0,1), \n","            ('D_94',0,1), ('D_96',0,1), ('D_103',0,1), ('D_106',0,23), ('D_107',0,3), ('D_108',0,1), ('D_109',0,1),('D_111',0,2),\n","            ('D_113',0,5), ('D_122',0,7), ('D_123',0,1), ('D_124',22,22), ('D_125',0,1), ('D_127',0,1), ('D_129',0,1), ('D_135',0,1),\n","            ('D_136',0,4), ('D_137',0,1), ('D_138',0,2), ('D_139',0,1), ('D_140',0,1), ('D_143',0,1), ('D_145',0,11), ('R_2',0,1), \n","            ('R_3',0,10), ('R_4',0,1), ('R_5',0,2),('R_8',0,1), ('R_9',0,6), ('R_10',0,1),('R_11',0,2), ('R_13',0,31), \n","            ('R_15',0,1), ('R_16',0,2), ('R_17',0,35), ('R_18',0,31),('R_19',0,1), ('R_20',0,1), ('R_21',0,1),('R_22',0,1), \n","            ('R_23',0,1), ('R_24',0,1), ('R_25',0,1), ('R_26',0,28), ('R_28',0,1), ('S_6',0,1), ('S_11',5,25), ('S_15',10/3,10),\n","            ('S_18',0,1), ('S_20',0,1),\n","            ('S_8',0,100), ('S_13',0,100) # these have overlap and should be treated differently\n","              ]\n","    \n","    def read_data(self, filename, update_columns=True, first_chunk_only=False, raw=False):\n","\n","        print(f'Reading Data ')\n","        if update_columns:\n","            #read all columns names and update lists\n","            self.all_columns = pd.read_csv(filename, index_col=False, nrows=0).columns.tolist()\n","            self.numeric_columns = [x for x in self.all_columns if x not in self.non_numeric_columns]\n","            #Types of features\n","            # self.delinquency_features = [col for col in self.all_columns if col.startswith('D_')]\n","            # self.spend_features = [col for col in self.all_columns if col.startswith('S_')]\n","            # self.payment_features = [col for col in self.all_columns if col.startswith('P_')]\n","            # self.balance_features = [col for col in self.all_columns if col.startswith('B_')]\n","            # self.risk_features = [col for col in self.all_columns if col.startswith('R_')]\n","            \n","            print(f'Calculating data sets aggregate statistics')\n","            aggregate_final = None\n","            final_column_na = None\n","            final_count_values = []\n","            drop_cuttoff = 0.35\n","            \n","            with pd.read_csv(filename, chunksize=400000) as reader: #self.chunksize\n","                for i, chunk in enumerate(reader):\n","                    \n","                    #calculate chunk aggregates\n","                    aggregate = chunk[self.numeric_columns].agg(['mean', 'count', 'min', 'max',squared_mean]).transpose()\n","                    #coutn number of na in chunk\n","                    chunk_column_na = chunk.agg(['count',missing_values]).transpose() \n","                    #get count of each value in categorical columns\n","                    chunk_count_values = dict()\n","                    for column in self.categorical_columns:\n","                        chunk_count_values[column] = chunk[column].value_counts().to_dict()\n","                    final_count_values.append(pd.DataFrame(chunk_count_values))\n","                    \n","                    del chunk\n","                    gc.collect()\n","                    \n","                    #update aggregates\n","                    if aggregate_final is not None:\n","                        aggregate_final['mean'] = (aggregate_final['mean']*aggregate_final['count']  + aggregate['mean']*aggregate['count'] ) / ( aggregate_final['count'] + aggregate['count'] )\n","                        aggregate_final['squared_mean'] = (aggregate_final['squared_mean']*aggregate_final['count']  + aggregate['squared_mean']*aggregate['count'] ) / ( aggregate_final['count'] + aggregate['count'] )\n","                        aggregate_final['count'] = aggregate_final['count'] + aggregate['count']\n","                        aggregate_final['min'] = pd.concat([aggregate_final['min'],  aggregate['min']], axis=1).min(axis=1)\n","                        aggregate_final['max'] = pd.concat([aggregate_final['max'],  aggregate['max']], axis=1).max(axis=1)\n","                    else:\n","                        aggregate_final = aggregate\n","                        \n","                    #update NA values\n","                    if final_column_na is not None:\n","                        final_column_na['count'] = final_column_na['count'] + chunk_column_na['count']\n","                        final_column_na['missing_values'] = final_column_na['missing_values'] + chunk_column_na['missing_values']\n","                        break\n","                    else:\n","                        final_column_na = chunk_column_na\n","                        \n","            #Na Values final \n","            final_column_na['percent_missing'] = final_column_na['missing_values'] / (final_column_na['missing_values'] +final_column_na['count'])\n","            self.columns_drop = list(final_column_na[final_column_na['percent_missing']> drop_cuttoff].reset_index()['index'])\n","            #Add redundant features to drop list \n","            self.columns_drop = list(set(self.columns_drop + ['D_103','D_139'])) #tied to D_107 and D_145 respectively\n","            \n","            #update meta data list for columns to remove columns that will be dropped\n","            self.categorical_columns = list(set(self.categorical_columns) - set(self.columns_drop ) )\n","            self.non_numeric_columns = list(set(self.non_numeric_columns) - set(self.columns_drop))\n","            self.all_columns = list(set(self.all_columns) - set(self.columns_drop))\n","            self.numeric_columns = list(set(self.numeric_columns ) - set(self.columns_drop))\n","            self.int_scaling = [x for x in self.int_scaling if x[0] not in self.columns_drop]\n","            \n","            #aggregates values final\n","            aggregate_final['var'] = aggregate_final['squared_mean'] - aggregate_final['mean']**2      \n","            aggregate_final['std'] = aggregate_final['var']**(1/2)\n","            aggregate_final = aggregate_final[~aggregate_final.index.isin(self.columns_drop)] #remove columns that will be dropped\n","            self.aggregate_stats  = aggregate_final\n","                       \n","            #merge all value counts\n","            final_count_values = pd.concat(final_count_values).groupby(level=0).sum()\n","            #remove columns that will be dropped\n","            final_count_values = final_count_values[~final_count_values.index.isin(self.columns_drop)]\n","            self.categorical_mode = final_count_values.idxmax().to_dict()\n","            #get dict of values to impute categorical values\n","            categorical_values = dict()\n","            for column in final_count_values.columns:\n","                mapping_dict = final_count_values[final_count_values[column] != 0].reset_index()['index'].to_dict()\n","                categorical_values[column] = { mapping_dict[x]:x for x in mapping_dict}\n","            self.categorical_encode = categorical_values\n","        print(f'Reading {filename} in chunks')\n","        if raw:\n","            return pd.read_csv(filename)\n","        else:\n","            output = []\n","            with pd.read_csv(filename, chunksize=self.chunksize) as reader:\n","                for i, chunk in enumerate(reader):\n","                    print(f'Reading chunk: {i+1}')\n","                    output.append(self.process_chunk(chunk))\n","                    memory_usage(\"GB\",1)\n","                    \n","                    if first_chunk_only:\n","                        break\n","                    gc.collect()\n","                    \n","            print(f'finished reading all chunks')\n","            gc.collect()\n","            self.previous_chunk_data = None\n","            print(f'combining all chunks')\n","            data = pd.concat(output, copy=False).sort_index().reset_index(drop=True)\n","            print(f'minimising data types')\n","            self.print_memory_usage(data, label='before')\n","            self.update_dtypes = self.compress(data)\n","            data = data.astype(self.update_dtypes)\n","            self.print_memory_usage(data, label='compressed')\n","            \n","            return data\n","\n","\n","\n","    def process_chunk(self, chunk):\n","\n","        ##Pre Processing of dataframe chunk to make sure customer ID is all processed at same time##\n","\n","        #Test to see if it is not last chunk and take last id from the DF incase it is over 2 chunks\n","        if len(chunk) >= self.chunksize:\n","            last_id_in_chunk = chunk['customer_ID'].iloc[-1]\n","            last_id_in_chunk_data = chunk[chunk['customer_ID']==last_id_in_chunk].copy()\n","            chunk = chunk.loc[chunk['customer_ID']!=last_id_in_chunk]\n","        else: \n","            last_id_in_chunk_data = None\n","\n","        #Check if any previous chunk data\n","        if self.previous_chunk_data is not None: \n","            chunk = pd.concat([self.previous_chunk_data,chunk])\n","        \n","\n","        self.previous_chunk_data = last_id_in_chunk_data\n","        \n","        #calculate NA aggregates\n","        x_na_aggregate = chunk.groupby(\"customer_ID\")[self.numeric_columns].agg([missing_values, missing_last_value])\n","        x_na_aggregate.columns = ['_'.join(x) for x in x_na_aggregate.columns]\n","\n","        #Drop columns with too many na\n","        chunk.drop(self.columns_drop, axis=1, inplace=True)\n","        \n","        #fill NA\n","        numeric_means = self.aggregate_stats['mean'].to_dict()\n","        numeric_means = { x:numeric_means[x] for x in numeric_means if  x.startswith('R_') or x.startswith('D_')}\n","        print('filling na')\n","        chunk.fillna(numeric_means, inplace=True)\n","        chunk.fillna(self.categorical_mode, inplace=True)\n","        print('filling 0')\n","        chunk.fillna(0, inplace=True) #fill remaining with 0\n","\n","        #encode categorical values\n","        chunk.replace(self.categorical_encode, inplace=True)\n","        \n","        #set numeric datatype\n","        update_dtypes_numeric = {x: self.numeric_dtype for x in self.numeric_columns}\n","        chunk = chunk.astype(update_dtypes_numeric)\n","        \n","        update_dtypes_categorical = {x: np.int16 for x in self.categorical_columns}\n","        chunk = chunk.astype(update_dtypes_categorical)\n","                \n","        #clip outliers to max/min values\n","        n_deviations_limit = 10\n","        upper_limit = list(self.aggregate_stats['mean'] + self.aggregate_stats['std'] * n_deviations_limit)\n","        lower_limit = list(self.aggregate_stats['mean'] - self.aggregate_stats['std'] * n_deviations_limit)\n","        chunk[self.numeric_columns] = chunk[self.numeric_columns].clip(lower_limit,upper_limit, axis=1)\n","        \n","        #Remove noise from float columns ie turn float -> int\n","        for column_conversion in self.int_scaling: #column_conversion -> (column_name, add_value, multiply_value)\n","            chunk[column_conversion[0]] = ((chunk[column_conversion[0]] +  column_conversion[1])*column_conversion[2]).round(0).astype(np.int16)\n","#             if column_conversion[0] =='S_13':\n","#                 print(chunk[column_conversion[0]].unique())\n","\n","        \n","        #create aggregates for each customer_id numeric columns\n","        print('Doing Aggregates')\n","        x_aggregate = chunk.groupby(\"customer_ID\")[self.numeric_columns].agg(['first', 'mean', 'std', 'min', 'max', 'last', lm_diff])\n","        x_aggregate.columns = ['_'.join(x) for x in x_aggregate.columns]\n","        #fill std columns with 0 incase of only one value as it results in nan\n","        std_fill_na_columns = {x:0 for x in x_aggregate.columns if '_std' in x}\n","        x_aggregate.fillna(std_fill_na_columns, inplace=True)\n","        \n","        #create aggregates for each customer_id categorical columns\n","        x_aggregate_category = chunk.groupby(\"customer_ID\")[self.categorical_columns].agg(['first', 'last']) #,'nunique''count',  removed because dont seem to add much value\n","        x_aggregate_category.columns = ['_'.join(x) for x in x_aggregate_category.columns]\n","        \n","        #Get the number of months a record has been active\n","        chunk['S_2'] = pd.to_datetime(chunk['S_2'])\n","        x_date = chunk.groupby(\"customer_ID\")['S_2'].agg(['first','last'])\n","        x_aggregate['months'] = (x_date['last'].dt.year  - x_date['first'].dt.year)*12 + (x_date['last'].dt.month  - x_date['first'].dt.month)+1\n","        \n","        del chunk, x_date\n","        gc.collect()\n","\n","        #Features for how metrics have changed over time \n","        for column in  x_aggregate:\n","            if 'first' in column:\n","                column_first = column\n","                column = column.replace('_first', '')\n","                column_last = column +'_last'\n","\n","                x_aggregate[column+'_change_sub'] = (x_aggregate[column_first] - x_aggregate[column_last])/ x_aggregate['months']\n","                x_aggregate[column+'_change_div'] = ((x_aggregate[column_first] / x_aggregate[column_last])/ x_aggregate['months']).replace([np.inf, -np.inf], np.nan).fillna(0)\n","        \n","        \n","        # #create ratio features between payment and spending\n","        # payment_features = [col.replace('_first', '') for col in x_aggregate if col.startswith('P_') and col.endswith('_first')]\n","        # spend_features = [col.replace('_first', '') for col in x_aggregate if col.startswith('S_') and col.endswith('_first')]\n","        \n","        # column_suffix = ['_mean', '_last']\n","        # for p_column in payment_features:\n","        #     for s_column in spend_features:\n","        #         column_name = f'{p_column}_{s_column}'\n","        #         for suffix in column_suffix:\n","        #             x_aggregate[column_name + '_ratio' + suffix] = (x_aggregate[p_column+suffix] / x_aggregate[s_column+suffix]).replace([np.inf, -np.inf], np.nan).fillna(0)\n","        #             x_aggregate[column_name + '_sub' + suffix] = (x_aggregate[p_column+suffix] - x_aggregate[s_column+suffix])\n","                \n","\n","        #create combination features\n","        features = ['B_3','B_1','B_37','B_9','B_2','B_7','B_18','D_48','D_44','D_39','P_2','P_3','P_4','R_1','R_2','R_3','S_3','S_23','S_7']\n","        column_suffix = '_last'\n","        for p_column in features:\n","            #print(p_column)\n","            for s_column in features:\n","                column_name = f'{p_column}_{s_column}'\n","                x_aggregate[column_name + '_multi' + column_suffix] = (x_aggregate[p_column+column_suffix] * x_aggregate[s_column+column_suffix])\n","                if p_column != s_column:\n","                    x_aggregate[column_name + '_ratio' + column_suffix] = (x_aggregate[p_column+column_suffix] / x_aggregate[s_column+column_suffix]).replace([np.inf, -np.inf], np.nan).fillna(0)\n","                    x_aggregate[column_name + '_sub' + column_suffix] = (x_aggregate[p_column+column_suffix] - x_aggregate[s_column+column_suffix])\n","\n","\n","        update_cols = {x: np.float32 for x in x_aggregate.select_dtypes(include=[float]).columns}\n","        x_aggregate = x_aggregate.astype(update_cols)\n","        \n","        #feature for has the category change over time\n","        for column in  x_aggregate_category:\n","            if 'first' in column:\n","                column_first = column\n","                column = column.replace('_first', '')\n","                column_last = column +'_last'\n","\n","                x_aggregate_category[column+'_change'] = (x_aggregate_category[column_first] == x_aggregate_category[column_last]).astype(np.int16)\n","                \n","        chunk = pd.concat([x_aggregate, x_aggregate_category, x_na_aggregate], axis=1)\n","        \n","        #drop first aggregate\n","        columns_drop = [x for x in chunk.columns if '_first' in x]\n","        chunk.drop(columns_drop, axis=1, inplace=True)\n","\n","        return chunk\n","    \n","    def print_memory_usage(self, data, metric='MB', label=''):\n","        metric_mapping = {'B':0, 'KB': 2, 'MB': 2, 'GB': 3}\n","        multiplier= metric_mapping[metric]\n","        memory = data.memory_usage().sum() / (1024**multiplier)\n","        print(f'Memory usage {label}: {memory:.2f}{metric}')\n","\n","    \n","    def compress(self, data):\n","        INT8_MIN    = np.iinfo(np.int8).min\n","        INT8_MAX    = np.iinfo(np.int8).max\n","        INT16_MIN   = np.iinfo(np.int16).min\n","        INT16_MAX   = np.iinfo(np.int16).max\n","        INT32_MIN   = np.iinfo(np.int32).min\n","        INT32_MAX   = np.iinfo(np.int32).max\n","\n","        FLOAT16_MIN = np.finfo(np.float16).min\n","        FLOAT16_MAX = np.finfo(np.float16).max\n","        FLOAT32_MIN = np.finfo(np.float32).min\n","        FLOAT32_MAX = np.finfo(np.float32).max\n","        column_dtypes = {}\n","        for col in data.columns:\n","            col_dtype = data[col][:100].dtype\n","\n","            if col_dtype != 'object':\n","                col_series = data[col]\n","                col_min = col_series.min()\n","                col_max = col_series.max()\n","\n","                if col_dtype == 'float64':\n","#                     if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n","#                         column_dtypes[col] = np.float16\n","#                     elif\n","                    if (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n","                        column_dtypes[col] = np.float32\n","                    else:\n","                        pass\n","\n","\n","                if col_dtype == 'int64':\n","                    if (col_min > INT8_MIN/2) and (col_max < INT8_MAX/2):\n","                        column_dtypes[col] = np.int8\n","                    elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n","                        column_dtypes[col] = np.int16\n","                    elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n","                        column_dtypes[col] = np.int32\n","                    else:\n","                        pass\n","        return column_dtypes\n","    \n","    \n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T04:12:43.209282Z","iopub.status.busy":"2022-07-28T04:12:43.208488Z","iopub.status.idle":"2022-07-28T04:13:51.899954Z","shell.execute_reply":"2022-07-28T04:13:51.898809Z","shell.execute_reply.started":"2022-07-28T04:12:43.209231Z"},"trusted":true},"outputs":[],"source":["# a_amex_helper = amex_helper(chunksize= 1000)\n","# x_input = a_amex_helper.read_data(filename, first_chunk_only=True)\n","# x_input.head(5)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-28T03:59:40.708090Z","iopub.status.busy":"2022-07-28T03:59:40.707750Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading Data \n","Calculating data sets aggregate statistics\n","Reading kaggle/input/amex-default-prediction/train_data.csv in chunks\n","Reading chunk: 1\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.0GB Memory used Total: 16.0GB\n","Reading chunk: 2\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.3GB Memory used Total: 15.9GB\n","Reading chunk: 3\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.6GB Memory used Total: 16.1GB\n","Reading chunk: 4\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 3.0GB Memory used Total: 16.1GB\n","Reading chunk: 5\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.6GB Memory used Total: 17.3GB\n","Reading chunk: 6\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.6GB Memory used Total: 15.9GB\n","Reading chunk: 7\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.7GB Memory used Total: 16.4GB\n","Reading chunk: 8\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.7GB Memory used Total: 16.8GB\n","Reading chunk: 9\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.7GB Memory used Total: 17.1GB\n","Reading chunk: 10\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.6GB Memory used Total: 17.4GB\n","Reading chunk: 11\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.7GB Memory used Total: 17.3GB\n","Reading chunk: 12\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.6GB Memory used Total: 17.4GB\n","Reading chunk: 13\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.7GB Memory used Total: 18.0GB\n","Reading chunk: 14\n","filling na\n","filling 0\n","Doing Aggregates\n","Memory used Process: 2.3GB Memory used Total: 17.7GB\n","finished reading all chunks\n","combining all chunks\n","minimising data types\n","Memory usage before: 4556.85MB\n","Memory usage compressed: 3897.74MB\n"]},{"data":{"text/plain":["0"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["a_amex_helper = amex_helper(chunksize= 400000)\n","x_input = a_amex_helper.read_data(filename, first_chunk_only=False)\n","y_input = pd.read_csv(filename_label)\n","y_input = y_input.loc[x_input.index, :]\n","gc.collect()"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-22T00:44:04.051290Z","iopub.status.busy":"2022-07-22T00:44:04.049844Z","iopub.status.idle":"2022-07-22T00:44:57.206378Z","shell.execute_reply":"2022-07-22T00:44:57.204984Z","shell.execute_reply.started":"2022-07-22T00:44:04.051223Z"},"trusted":true},"outputs":[],"source":["x_input.to_parquet('x_input.parquet')\n","y_input.to_parquet('y_input.parquet')\n","\n","# Store data (serialize)\n","with open('helper_object.pickle', 'wb') as handle:\n","    pickle.dump(a_amex_helper, handle, protocol=pickle.HIGHEST_PROTOCOL)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"}}},"nbformat":4,"nbformat_minor":4}
